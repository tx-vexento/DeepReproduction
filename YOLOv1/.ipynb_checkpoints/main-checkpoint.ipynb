{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1715fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65e3bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# 卷积朴素实现\n",
    "def conv2d(input, weight, bias=None, stride=1, padding=0):\n",
    "    batch_size, in_channels, in_height, in_width = input.size()\n",
    "    out_channels, _, kernel_height, kernel_width = weight.size()\n",
    "    out_height = int((in_height + 2 * padding - kernel_height) / stride) + 1\n",
    "    out_width = int((in_width + 2 * padding - kernel_width) / stride) + 1\n",
    "    padded_input = torch.nn.functional.pad(input, (padding, padding, padding, padding))\n",
    "    output = torch.zeros(batch_size, out_channels, out_height, out_width)\n",
    "    for b in range(batch_size):\n",
    "        for c_out in range(out_channels):\n",
    "            for h_out in range(out_height):\n",
    "                for w_out in range(out_width):\n",
    "                    h_start = h_out * stride\n",
    "                    w_start = w_out * stride\n",
    "                    h_end = h_start + kernel_height\n",
    "                    w_end = w_start + kernel_width\n",
    "                    input_patch = padded_input[b, :, h_start:h_end, w_start:w_end]\n",
    "                    output[b, c_out, h_out, w_out] = torch.sum(input_patch * weight[c_out]) + (bias[c_out] if bias is not None else 0)\n",
    "    return output\n",
    "\"\"\"\n",
    "\n",
    "# \"\"\"\n",
    "def conv2d(inputs, weight, bias=None, stride=1, padding=0):\n",
    "    padded_input = torch.nn.functional.pad(inputs, (padding, padding, padding, padding))\n",
    "    output = torch.nn.functional.conv2d(padded_input, weight, bias=bias, stride=stride)\n",
    "    return output\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1621f122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef max_pool2d(input, kernel_size, stride=1, padding=0):\\n    output = torch.nn.functional.max_pool2d(input, kernel_size, stride=stride, padding=padding)\\n    return output\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# max 池化朴素实现\n",
    "def max_pool2d(inputs, kernel_size, stride=None, padding=0):\n",
    "    batch_size, channels, height, width = inputs.size()\n",
    "    output_height = int((height + 2 * padding - kernel_size) / stride) + 1\n",
    "    output_width = int((width + 2 * padding - kernel_size) / stride) + 1\n",
    "    unfolded = torch.nn.functional.unfold(\n",
    "        inputs,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation=1,\n",
    "        padding=padding,\n",
    "        stride=stride\n",
    "    )\n",
    "    unfolded = unfolded.view(batch_size, channels, -1, output_height, output_width)\n",
    "    output, _ = torch.max(unfolded, dim=2)\n",
    "    return output\n",
    "# \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def max_pool2d(input, kernel_size, stride=1, padding=0):\n",
    "    output = torch.nn.functional.max_pool2d(input, kernel_size, stride=stride, padding=padding)\n",
    "    return output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a797ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, p):\n",
    "    return X * (torch.rand_like(X) > p).float() / (1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "173f1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "def leaky_reLu(x, negative_slope=0.01):\n",
    "    return torch.where(x >= 0, x, negative_slope * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c67abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58e5bff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef AdaptiveAvgPool2d(x, output_size):\\n    return torch.nn.AdaptiveAvgPool2d(output_size)(x)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# 全局平均汇聚层朴素实现\n",
    "def AdaptiveAvgPool2d(x, output_size):\n",
    "    batch_size, channels, height, width = x.size()\n",
    "    output_h, output_w = output_size\n",
    "    stride_h = height // output_h\n",
    "    stride_w = width // output_w\n",
    "    output = []\n",
    "    for i in range(output_h):\n",
    "        for j in range(output_w):\n",
    "            h_start = i * stride_h\n",
    "            h_end = min(h_start + stride_h, height)\n",
    "            w_start = j * stride_w\n",
    "            w_end = min(w_start + stride_w, width)\n",
    "            pool_region = x[:, :, h_start:h_end, w_start:w_end]\n",
    "            pool_avg = torch.mean(pool_region, dim=(2, 3))\n",
    "            output.append(pool_avg)\n",
    "    output = torch.stack(output, dim=2)\n",
    "    output = output.view(batch_size, channels, output_h, output_w)\n",
    "\n",
    "    return output\n",
    "# \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def AdaptiveAvgPool2d(x, output_size):\n",
    "    return torch.nn.AdaptiveAvgPool2d(output_size)(x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e47d8f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as tvmodel\n",
    "resnet = tvmodel.resnet34(pretrained=True)\n",
    "resnet_out_channel = resnet.fc.in_features\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-2])\n",
    "GL_CLASSES = ['person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "           'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "           'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor']\n",
    "GL_NUMBBOX = 2\n",
    "GL_NUMGRID = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4f7c371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1Net:\n",
    "    def __init__(self):\n",
    "        self._init_params()\n",
    "        \n",
    "    def _init_params(self):\n",
    "        \"\"\"初始化参数\"\"\"\n",
    "        def normal(shape):\n",
    "            return torch.nn.init.xavier_uniform_(torch.empty(shape))\n",
    "        \"\"\"输出通道 * 输入通道 * 卷积核边长 * 卷积核边长\"\"\"\n",
    "        \"\"\"卷积层\"\"\"\n",
    "        W_1 = normal((1024, resnet_out_channel, 3, 3)); b_1 = torch.zeros(1024, device=device)\n",
    "        W_2 = normal((1024, 1024, 3, 3)); b_2 = torch.zeros(1024, device=device)\n",
    "        W_3 = normal((1024, 1024, 3, 3)); b_3 = torch.zeros(1024, device=device)\n",
    "        W_4 = normal((1024, 1024, 3, 3)); b_4 = torch.zeros(1024, device=device)\n",
    "        convs = [W_1, b_1, W_2, b_2, W_3, b_3, W_4, b_4]\n",
    "        W_1 = normal((GL_NUMGRID * GL_NUMGRID * 1024, 4096)); b_1 = torch.zeros(4096, device=device)\n",
    "        W_2 = normal((4096, GL_NUMGRID * GL_NUMGRID * (5 * GL_NUMBBOX + len(GL_CLASSES)))); \n",
    "        b_2 = torch.zeros(GL_NUMGRID * GL_NUMGRID * (5 * GL_NUMBBOX + len(GL_CLASSES)), device=device)\n",
    "        linears = [W_1, b_1, W_2, b_2]\n",
    "        self.params = [convs, linears]; self.flt_params = []\n",
    "        for params in [convs, linears]:\n",
    "            for param in params:\n",
    "                self.flt_params.append(param)\n",
    "        for param in self.flt_params:\n",
    "            param.requires_grad_(True)        \n",
    "\n",
    "    def _forward(self, X):\n",
    "        \"\"\"推理函数\"\"\"\n",
    "        convs, linears = self.params\n",
    "        X = resnet(X)\n",
    "        W_1, b_1, W_2, b_2, W_3, b_3, W_4, b_4 = convs\n",
    "        X = relu(conv2d(X, W_1, b_1, padding=1))\n",
    "        X = relu(conv2d(X, W_2, b_2, stride=2, padding=1))\n",
    "        X = relu(conv2d(X, W_3, b_3, padding=1))\n",
    "        X = relu(conv2d(X, W_4, b_4, padding=1))\n",
    "        X = X.reshape(X.size(0), -1)\n",
    "        W_1, b_1, W_2, b_2 = linears\n",
    "        X = relu(X @ W_1 + b_1)\n",
    "        X = sigmoid(X @ W_2 + b_2)\n",
    "        Y = X.reshape(-1, (5 * GL_NUMBBOX + len(GL_CLASSES)), GL_NUMGRID, GL_NUMGRID)\n",
    "        return Y\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self._forward(X)\n",
    "    \n",
    "    def update(self, X, y, lr):\n",
    "        \"\"\"更新函数\"\"\"\n",
    "        y_hat = self._forward(X)\n",
    "        l = self._loss(y_hat, y)\n",
    "        l.mean().backward()\n",
    "        self.grad_clipping(1)\n",
    "        \"\"\"sgd\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param in self.flt_params:\n",
    "                param -= lr * param.grad / X.shape[0]\n",
    "                param.grad.zero_()\n",
    "        return l\n",
    "            \n",
    "    def _loss(self, pred, labels):\n",
    "        \"\"\"\n",
    "        pred: (batchsize, 30, 7, 7) 的网络输出数据\n",
    "        labels: (batchsize, 30, 7, 7) 的样本标签数据\n",
    "        return: 当前批次样本的平均损失\n",
    "        \"\"\"\n",
    "        num_gridx, num_gridy = labels.size()[-2:]  \n",
    "        num_b = 2  # 每个网格的 bbox 数量\n",
    "        num_cls = 20  # 类别数量\n",
    "        noobj_confi_loss = 0.  # 不含目标的网格损失(只有置信度损失)\n",
    "        coor_loss = 0.  # 含有目标的bbox的坐标损失\n",
    "        obj_confi_loss = 0.  # 含有目标的bbox的置信度损失\n",
    "        class_loss = 0.  # 含有目标的网格的类别损失\n",
    "        n_batch = labels.size()[0]  # batchsize的大小\n",
    "        \n",
    "        def calculate_iou(bbox1, bbox2):\n",
    "            \"\"\"计算 bbox1 = (x1, y1, x2, y2) 和 bbox2 = (x3, y3, x4, y4) 两个 bbox 的 iou\"\"\"\n",
    "            x1, y1, x2, y2 = bbox1\n",
    "            x3, y3, x4, y4 = bbox2\n",
    "            intersect_width = max(0, min(x2, x4) - max(x1, x3))\n",
    "            intersect_height = max(0, min(y2, y4) - max(y1, y3))\n",
    "            intersection_area = intersect_width * intersect_height\n",
    "            area1 = (x2 - x1) * (y2 - y1)\n",
    "            area2 = (x4 - x3) * (y4 - y3)\n",
    "            iou = intersection_area / (area1 + area2 - intersection_area) if intersection_area > 0 else 0\n",
    "            return iou\n",
    "        for i in range(n_batch): \n",
    "            for n in range(7):\n",
    "                for m in range(7):\n",
    "                    if labels[i, 4, m, n] == 1: # 如果包含物体\n",
    "                        bbox1_pred_xyxy = ((pred[i, 0, m, n] + n) / num_gridx - pred[i, 2, m, n] / 2,\n",
    "                                           (pred[i, 1, m, n] + m) / num_gridy - pred[i, 3, m, n] / 2,\n",
    "                                           (pred[i, 0, m, n] + n) / num_gridx + pred[i, 2, m, n] / 2,\n",
    "                                           (pred[i, 1, m, n] + m) / num_gridy + pred[i, 3, m, n] / 2)\n",
    "                        bbox2_pred_xyxy = ((pred[i, 5, m, n] + n) / num_gridx - pred[i, 7, m, n] / 2,\n",
    "                                           (pred[i, 6, m, n] + m) / num_gridy - pred[i, 8, m, n] / 2,\n",
    "                                           (pred[i, 5, m, n] + n) / num_gridx + pred[i, 7, m, n] / 2,\n",
    "                                           (pred[i, 6, m, n] + m) / num_gridy + pred[i, 8, m, n] / 2)\n",
    "                        bbox_gt_xyxy = ((labels[i, 0, m, n] + n) / num_gridx - labels[i, 2, m, n] / 2,\n",
    "                                        (labels[i, 1, m, n] + m) / num_gridy - labels[i, 3, m, n] / 2,\n",
    "                                        (labels[i, 0, m, n] + n) / num_gridx + labels[i, 2, m, n] / 2,\n",
    "                                        (labels[i, 1, m, n] + m) / num_gridy + labels[i, 3, m, n] / 2)\n",
    "                        iou1 = calculate_iou(bbox1_pred_xyxy, bbox_gt_xyxy)\n",
    "                        iou2 = calculate_iou(bbox2_pred_xyxy, bbox_gt_xyxy)\n",
    "                        coor_loss += 5 * (torch.sum((pred[i, [0, 1], m, n] - labels[i, [0, 1], m, n]) ** 2)\n",
    "                                          + torch.sum((pred[i, [2, 3], m, n].sqrt() - \n",
    "                                                       labels[i, [2, 3], m, n].sqrt()) ** 2))\n",
    "                        obj_confi_loss += (pred[i, 4, m, n] - iou1) ** 2 \\\n",
    "                                        if iou1 >= iou2 else (pred[i, 9, m, n] - iou2) ** 2\n",
    "                        noobj_confi_loss += 0.5 * ((pred[i, 9, m, n] - iou2) ** 2 \\\n",
    "                                        if iou1 >= iou2 else (pred[i, 4, m, n] - iou1) ** 2)\n",
    "                        class_loss += torch.sum((pred[i, 10:, m, n] - labels[i, 10:, m, n]) ** 2)\n",
    "                    else:\n",
    "                        noobj_confi_loss += 0.5 * torch.sum(pred[i, [4, 9], m, n] ** 2)\n",
    "        loss = coor_loss + obj_confi_loss + noobj_confi_loss + class_loss\n",
    "        return loss / n_batch\n",
    "    \n",
    "    def grad_clipping(self, theta):\n",
    "        norm = torch.sqrt(sum([torch.sum(p ** 2) for p in self.flt_params]))\n",
    "        if norm > theta:\n",
    "            for param in self.flt_params:\n",
    "                param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "959cc731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "82d9269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, seed=None, mode=\"train\", train_val_ratio=0.9, trans=None):\n",
    "        random.seed(seed)\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.mode = mode\n",
    "        img_list_txt = os.path.join(dataset_dir, mode.replace(\"val\", \"train\") + \".txt\")\n",
    "        label_csv = os.path.join(dataset_dir, mode.replace(\"val\", \"train\") + \".csv\")\n",
    "        self.img_list = open(img_list_txt).read().splitlines()\n",
    "        self.label = np.loadtxt(label_csv, dtype=np.float32)\n",
    "        self.num_all_data = len(self.img_list)\n",
    "        all_ids = list(range(self.num_all_data))\n",
    "        num_train = int(train_val_ratio * self.num_all_data)\n",
    "        self.use_ids = all_ids[:num_train] if mode == \"train\" else all_ids[num_train:]\n",
    "        self.trans = trans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        id = self.use_ids[item]\n",
    "        label = torch.tensor(self.label[id, :])\n",
    "        img_path = self.img_list[id]\n",
    "        img = Image.open(img_path)\n",
    "        if self.trans is None:\n",
    "            trans = transforms.Compose([transforms.ToTensor(),])\n",
    "        else:\n",
    "            trans = self.trans\n",
    "        img = trans(img)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ee0d82da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1703"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch, batch_size, lr, num_epochs = 50, 5, 0.1, 1\n",
    "dataset_dir = \"../DataSet/VOCdevkit/VOC2012/voc2012_forYolov1/\"\n",
    "dataset = MyDataset(dataset_dir)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "08606964",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = YOLOv1Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "927d924c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 1 loss 18.734579\n",
      "i 2 loss 18.448498\n",
      "i 3 loss 17.891035\n",
      "i 4 loss 17.330986\n",
      "i 5 loss 17.035566\n",
      "i 6 loss 16.567728\n",
      "i 7 loss 16.269590\n",
      "i 8 loss 15.972232\n",
      "i 9 loss 15.714748\n",
      "i 10 loss 15.333399\n",
      "i 11 loss 15.174784\n",
      "i 12 loss 14.869288\n",
      "i 13 loss 14.533778\n",
      "i 14 loss 14.218907\n",
      "i 15 loss 14.013656\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mview(batch_size, GL_NUMGRID, GL_NUMGRID, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m metrics[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l \u001b[38;5;241m*\u001b[39m batch_size; metrics[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m loss \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, metrics[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m metrics[\u001b[38;5;241m1\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[116], line 48\u001b[0m, in \u001b[0;36mYOLOv1Net.update\u001b[1;34m(self, X, y, lr)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, lr):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"更新函数\"\"\"\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(y_hat, y)\n\u001b[0;32m     50\u001b[0m     l\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[116], line 38\u001b[0m, in \u001b[0;36mYOLOv1Net._forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     36\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     37\u001b[0m W_1, b_1, W_2, b_2 \u001b[38;5;241m=\u001b[39m linears\n\u001b[1;32m---> 38\u001b[0m X \u001b[38;5;241m=\u001b[39m relu(\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW_1\u001b[49m \u001b[38;5;241m+\u001b[39m b_1)\n\u001b[0;32m     39\u001b[0m X \u001b[38;5;241m=\u001b[39m sigmoid(X \u001b[38;5;241m@\u001b[39m W_2 \u001b[38;5;241m+\u001b[39m b_2)\n\u001b[0;32m     40\u001b[0m Y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m GL_NUMBBOX \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(GL_CLASSES)), GL_NUMGRID, GL_NUMGRID)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    metrics = [0, 0]\n",
    "    for i, (X, y) in enumerate(train_dataloader):\n",
    "        y = y.view(batch_size, GL_NUMGRID, GL_NUMGRID, -1)\n",
    "        y = y.permute(0, 3, 1, 2)\n",
    "        l = net.update(X, y, lr=lr)\n",
    "        metrics[0] += l * batch_size; metrics[1] += batch_size\n",
    "        print('i %d loss %f' % (i + 1, metrics[0] / metrics[1]))\n",
    "    print('epoch %d loss %f' % (epoch + 1, metrics[0] / metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72c62b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = '../YOLOv1/weights/net.pkl'\n",
    "torch.save(net, weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d75a403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "33f917b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_1cls(dets, thresh):\n",
    "    \"\"\"\n",
    "    单类别NMS\n",
    "    dets: ndarray, nx5, dets[i, 0:4] 分别是 bbox 坐标；dets[i, 4] 是置信度 score\n",
    "    thresh: NMS 算法设置的 iou 阈值\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2, scores = dets[:, 0], dets[:, 1], dets[:, 2], dets[:, 3], dets[:, 4]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(iou <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "    return keep\n",
    "\n",
    "def nms_multi_cls(dets, thresh, n_cls):\n",
    "    \"\"\"\n",
    "    多类别的NMS算法\n",
    "    dets: ndarray, nx6, dets[i, 0:4] 是 bbox 坐标；dets[i, 4] 是置信度 score；dets[i, 5] 是类别序号；\n",
    "    thresh: NMS 算法的阈值；\n",
    "    n_cls: 是类别总数\n",
    "    \"\"\"\n",
    "    keeps_index = []\n",
    "    for i in range(n_cls):\n",
    "        order_i = np.where(dets[:, 5] == i)[0]\n",
    "        det = dets[dets[:, 5] == i, 0:5]\n",
    "        if det.shape[0] == 0:\n",
    "            keeps_index.append([])\n",
    "            continue\n",
    "        keep = nms_1cls(det, thresh)\n",
    "        keeps_index.append(order_i[keep])\n",
    "    return keeps_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "672eb08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2bbox(matrix):\n",
    "    \"\"\"\n",
    "    将网络输出的 7*7*30 的数据转换为 bbox 的 (98, 25) 的格式，然后再将 NMS 处理后的结果返回\n",
    "    matrix: 注意，输入的数据中，bbox坐标的格式是 (px,py,w,h)，需要转换为 (x1,y1,x2,y2) 的格式再输入NMS\n",
    "    return: 返回NMS处理后的结果,bboxes.shape = (-1, 6), 0:4 是(x1,y1,x2,y2), 4是conf， 5是cls\n",
    "    \"\"\"\n",
    "    if matrix.size()[0:2] != (7,7):\n",
    "        raise ValueError(\"Error: Wrong labels size: \", matrix.size(), \" != (7,7)\")\n",
    "    matrix = matrix.numpy()\n",
    "    bboxes = np.zeros((98, 6))\n",
    "    matrix = matrix.reshape(49, -1)\n",
    "    bbox = matrix[:, :10].reshape(98, 5)\n",
    "    r_grid = np.array(list(range(7)))\n",
    "    r_grid = np.repeat(r_grid, repeats=14, axis=0)\n",
    "    c_grid = np.array(list(range(7)))\n",
    "    c_grid = np.repeat(c_grid, repeats=2, axis=0)[np.newaxis, :]\n",
    "    c_grid = np.repeat(c_grid, repeats=7, axis=0).reshape(-1)\n",
    "    bboxes[:, 0] = np.maximum((bbox[:, 0] + c_grid) / 7.0 - bbox[:, 2] / 2.0, 0)\n",
    "    bboxes[:, 1] = np.maximum((bbox[:, 1] + r_grid) / 7.0 - bbox[:, 3] / 2.0, 0)\n",
    "    bboxes[:, 2] = np.minimum((bbox[:, 0] + c_grid) / 7.0 + bbox[:, 2] / 2.0, 1)\n",
    "    bboxes[:, 3] = np.minimum((bbox[:, 1] + r_grid) / 7.0 + bbox[:, 3] / 2.0, 1)\n",
    "    bboxes[:, 4] = bbox[:, 4]\n",
    "    cls = np.argmax(matrix[:, 10:], axis=1)\n",
    "    cls = np.repeat(cls, repeats=2, axis=0)\n",
    "    bboxes[:, 5] = cls\n",
    "    keepid = nms_multi_cls(bboxes, thresh=0.01, n_cls=20)\n",
    "    ids = []\n",
    "    for x in keepid:\n",
    "        ids = ids + list(x)\n",
    "    ids = sorted(ids)\n",
    "    return bboxes[ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "332c7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(img, bbox):\n",
    "    \"\"\"\n",
    "    根据bbox的信息在图像上绘制 bounding box\n",
    "    :param img: 绘制bbox的图像\n",
    "    :param bbox: 是(n,6)的尺寸，0:4是(x1,y1,x2,y2), 4是conf， 5是cls\n",
    "    \"\"\"\n",
    "    h, w = img.shape[0:2]\n",
    "    n = bbox.shape[0]\n",
    "    for i in range(n):\n",
    "        confidence = bbox[i, 4]\n",
    "        if confidence<0.2:\n",
    "            continue\n",
    "        p1 = (int(w * bbox[i, 0]), int(h * bbox[i, 1]))\n",
    "        p2 = (int(w * bbox[i, 2]), int(h * bbox[i, 3]))\n",
    "        cls_name = GL_CLASSES[int(bbox[i, 5])]\n",
    "        print(cls_name, p1, p2)\n",
    "        cv2.rectangle(img, p1, p2, COLOR[int(bbox[i, 5])])\n",
    "        cv2.putText(img, cls_name, p1, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))\n",
    "        cv2.putText(img, str(confidence), (p1[0],p1[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))\n",
    "    cv2.imshow(\"bbox\", img)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046f093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b23b685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007_000027.jpg torch.Size([1, 3, 448, 448])\n",
      "person (0, 0) (116, 174)\n",
      "horse (0, 0) (222, 184)\n",
      "cat (235, 0) (448, 144)\n",
      "bicycle (330, 0) (448, 167)\n",
      "bird (0, 0) (238, 261)\n",
      "sheep (303, 0) (448, 205)\n",
      "dog (200, 62) (380, 254)\n",
      "diningtable (0, 121) (100, 299)\n",
      "train (160, 62) (422, 359)\n",
      "bus (63, 124) (276, 438)\n",
      "boat (95, 158) (334, 400)\n",
      "chair (160, 180) (414, 391)\n",
      "pottedplant (64, 222) (238, 448)\n",
      "sofa (0, 337) (138, 448)\n",
      "tvmonitor (12, 324) (184, 448)\n",
      "bottle (105, 350) (318, 448)\n",
      "car (130, 342) (448, 448)\n",
      "cow (235, 302) (448, 448)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "COLOR = [(255,0,0),(255,125,0),(255,255,0),(255,0,125),(255,0,250),\n",
    "         (255,125,125),(255,125,250),(125,125,0),(0,255,125),(255,0,0),\n",
    "         (0,0,255),(125,0,255),(0,125,255),(0,255,255),(125,125,255),\n",
    "         (0,255,0),(125,255,125),(255,255,255),(100,100,100),(0,0,0),]  # 用来标识20个类别的bbox颜色，可自行设定\n",
    "test_image_dir = './Test_Images/'\n",
    "img_list = os.listdir(test_image_dir)\n",
    "trans = transforms.Compose([transforms.ToTensor(),])\n",
    "for img_name in img_list:\n",
    "    img_path = os.path.join(test_image_dir, img_name)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = trans(img)\n",
    "    img = torch.unsqueeze(img, dim=0)\n",
    "    print(img_name, img.shape)\n",
    "    preds = torch.squeeze(net(img), dim=0).detach().cpu()\n",
    "    preds = preds.permute(1,2,0)\n",
    "    bbox = labels2bbox(preds)\n",
    "    draw_img = cv2.imread(img_path)\n",
    "    draw_bbox(draw_img, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e901f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d33fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8291bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2cd12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f39b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec09f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba90be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
