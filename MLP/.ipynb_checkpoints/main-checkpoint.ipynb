{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0d13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math, torch, random\n",
    "from torch import nn, tensor, arange, randn, zeros, tanh, mm\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad66c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs):\n",
    "        self.num_inputs, self.num_hiddens, self.num_outputs = num_inputs, num_hiddens, num_outputs\n",
    "        self.params = self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        num_inputs = self.num_inputs; num_hiddens = self.num_hiddens; num_outputs = self.num_outputs\n",
    "        def normal(shape):\n",
    "            return torch.randn(size=shape, device=device) * 0.01\n",
    "        W_xh = normal((num_inputs, num_hiddens)); b_h = torch.zeros(num_hiddens, device=device)\n",
    "        W_hy = normal((num_hiddens, num_outputs)); b_y = torch.zeros(num_outputs, device=device)\n",
    "        W = normal((num_inputs, num_outputs)); b = torch.zeros(num_outputs, device=device)\n",
    "        params = [W, b]\n",
    "        for param in params:\n",
    "            param.requires_grad_(True)\n",
    "        return params\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        W, b = self.params\n",
    "        for X in inputs:\n",
    "            Y = torch.tanh(X @ W + b)\n",
    "            outputs.append(Y)\n",
    "        return torch.cat(outputs, dim=0)\n",
    "    \n",
    "    def sgd(self, lr, batch_size):\n",
    "        with torch.no_grad():\n",
    "            for param in self.params:\n",
    "                param -= lr * param.grad / batch_size\n",
    "                param.grad.zero_()\n",
    "                \n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def grad_clipping(self, theta):\n",
    "        norm = torch.sqrt(sum([torch.sum(p) for p in self.params]))\n",
    "        if norm > theta:\n",
    "            for p in self.params:\n",
    "                p.grad[:] *= theta / norm\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1c0f729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5adb5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, device = 8, 'cpu'\n",
    "num_epochs, num_inputs, num_hiddens, num_outputs = 100, 2, 256, 1\n",
    "net = MLP(num_inputs, num_hiddens, num_outputs)\n",
    "loss = torch.nn.MSELoss()\n",
    "updater = torch.optim.SGD(net.params, lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ffdf62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 26.730768\n",
      "epoch 2 loss 26.557814\n",
      "epoch 3 loss 26.540184\n",
      "epoch 4 loss 26.532017\n",
      "epoch 5 loss 26.527061\n",
      "epoch 6 loss 26.523800\n",
      "epoch 7 loss 26.521450\n",
      "epoch 8 loss 26.519712\n",
      "epoch 9 loss 26.518322\n",
      "epoch 10 loss 26.517250\n",
      "epoch 11 loss 26.516363\n",
      "epoch 12 loss 26.515581\n",
      "epoch 13 loss 26.514967\n",
      "epoch 14 loss 26.514442\n",
      "epoch 15 loss 26.513983\n",
      "epoch 16 loss 26.513567\n",
      "epoch 17 loss 26.513229\n",
      "epoch 18 loss 26.512897\n",
      "epoch 19 loss 26.512598\n",
      "epoch 20 loss 26.512379\n",
      "epoch 21 loss 26.512144\n",
      "epoch 22 loss 26.511950\n",
      "epoch 23 loss 26.511763\n",
      "epoch 24 loss 26.511602\n",
      "epoch 25 loss 26.511440\n",
      "epoch 26 loss 26.511314\n",
      "epoch 27 loss 26.511168\n",
      "epoch 28 loss 26.511049\n",
      "epoch 29 loss 26.510954\n",
      "epoch 30 loss 26.510853\n",
      "epoch 31 loss 26.510759\n",
      "epoch 32 loss 26.510668\n",
      "epoch 33 loss 26.510586\n",
      "epoch 34 loss 26.510508\n",
      "epoch 35 loss 26.510429\n",
      "epoch 36 loss 26.510378\n",
      "epoch 37 loss 26.510311\n",
      "epoch 38 loss 26.510254\n",
      "epoch 39 loss 26.510201\n",
      "epoch 40 loss 26.510149\n",
      "epoch 41 loss 26.510099\n",
      "epoch 42 loss 26.510061\n",
      "epoch 43 loss 26.510029\n",
      "epoch 44 loss 26.509968\n",
      "epoch 45 loss 26.509939\n",
      "epoch 46 loss 26.509899\n",
      "epoch 47 loss 26.509865\n",
      "epoch 48 loss 26.509838\n",
      "epoch 49 loss 26.509819\n",
      "epoch 50 loss 26.509769\n",
      "epoch 51 loss 26.509760\n",
      "epoch 52 loss 26.509729\n",
      "epoch 53 loss 26.509687\n",
      "epoch 54 loss 26.509689\n",
      "epoch 55 loss 26.509661\n",
      "epoch 56 loss 26.509640\n",
      "epoch 57 loss 26.509604\n",
      "epoch 58 loss 26.509621\n",
      "epoch 59 loss 26.509577\n",
      "epoch 60 loss 26.509539\n",
      "epoch 61 loss 26.509542\n",
      "epoch 62 loss 26.509518\n",
      "epoch 63 loss 26.509497\n",
      "epoch 64 loss 26.509478\n",
      "epoch 65 loss 26.509485\n",
      "epoch 66 loss 26.509460\n",
      "epoch 67 loss 26.509460\n",
      "epoch 68 loss 26.509420\n",
      "epoch 69 loss 26.509445\n",
      "epoch 70 loss 26.509430\n",
      "epoch 71 loss 26.509348\n",
      "epoch 72 loss 26.509426\n",
      "epoch 73 loss 26.509403\n",
      "epoch 74 loss 26.509371\n",
      "epoch 75 loss 26.509375\n",
      "epoch 76 loss 26.509357\n",
      "epoch 77 loss 26.509375\n",
      "epoch 78 loss 26.509338\n",
      "epoch 79 loss 26.509352\n",
      "epoch 80 loss 26.509346\n",
      "epoch 81 loss 26.509336\n",
      "epoch 82 loss 26.509296\n",
      "epoch 83 loss 26.509319\n",
      "epoch 84 loss 26.509264\n",
      "epoch 85 loss 26.509298\n",
      "epoch 86 loss 26.509302\n",
      "epoch 87 loss 26.509308\n",
      "epoch 88 loss 26.509283\n",
      "epoch 89 loss 26.509295\n",
      "epoch 90 loss 26.509272\n",
      "epoch 91 loss 26.509274\n",
      "epoch 92 loss 26.509260\n",
      "epoch 93 loss 26.509258\n",
      "epoch 94 loss 26.509251\n",
      "epoch 95 loss 26.509272\n",
      "epoch 96 loss 26.509268\n",
      "epoch 97 loss 26.509256\n",
      "epoch 98 loss 26.509241\n",
      "epoch 99 loss 26.509232\n",
      "epoch 100 loss 26.509230\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    state = None; metrics = [0, 0]\n",
    "    for X, Y in data_iter(batch_size, features, labels):\n",
    "        y_hat = net(X)\n",
    "        y = Y.T.reshape(-1)\n",
    "        l = loss(y_hat, y)\n",
    "        l.backward()\n",
    "        net.sgd(1, batch_size)\n",
    "        metrics[0] += l * y.numel(); metrics[1] += y.numel()\n",
    "    print('epoch %d loss %f' % (epoch + 1, metrics[0] / metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a429ecbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in data_iter(batch_size, features, labels):\n",
    "    print(X.shape, Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49b48e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7154], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(tensor([[1.0, 2.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3eed06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = net.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7f9d069d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1949],\n",
       "        [-3.7460]], requires_grad=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6e0316a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.6577], requires_grad=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "15622f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bf8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
